# -*- coding: utf-8 -*-
"""190451P-ML_Lab_1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BTuA0qJ_MGBnL3_PH66MY2IwlqFitDQb

# **Data Importing**
"""

#importing data
import pandas as pd
from google.colab import drive
drive.mount('/content/drive')
train_path = '/content/drive/My Drive/ML_Lab1/train.csv'
valid_path = '/content/drive/My Drive/ML_Lab1/valid.csv'
test_path = '/content/drive/My Drive/ML_Lab1/test.csv'
train = pd.read_csv(train_path)
valid = pd.read_csv(valid_path)
test = pd.read_csv(test_path)
original_train = train.copy()
original_valid = train.copy()
original_test = test.copy()

"""# **Data Imputing**

Checking if there is any null values in the dataset
"""

train.isnull().sum()

"""Considering the fact that only 480/28520 (approximately 1.68%) values are null in the Speaker Age colum which is relatively small and does not significantly affect sample size, I considered imputation to fill out the null values by the mean of the age"""

mean_age = int(train['label_2'].mean())
train['label_2'].fillna(mean_age, inplace=True)
train['label_2'] = train['label_2'].astype(int)
train.head()

"""Checking if there is any null values in the valid dataset"""

valid.isnull().sum()

mean_age = int(valid['label_2'].mean())
valid['label_2'].fillna(mean_age, inplace=True)
valid['label_2'] = valid['label_2'].astype(int)
valid.head()

valid.isnull().sum()

"""# **Classification and Regression Models**

Decision Tree Classification Model
"""

from sklearn import tree

def decisionTreeClassifierScore(train_input , valid_input , train_output , valid_output):
  model = tree.DecisionTreeClassifier()
  model.fit(train_input , train_output)
  model_score = model.score(valid_input, valid_output)
  return model_score

"""KNN Classification Model"""

from sklearn.neighbors import KNeighborsClassifier
from math import sqrt
from sklearn.metrics import accuracy_score

def knnClassification(train_input, train_output, valid_input, valid_output , k):
    #k = int(sqrt(sqrt(len(train_input))))
    knn = KNeighborsClassifier(n_neighbors=k)
    knn.fit(train_input, train_output)
    y_pred = knn.predict(valid_input)
    accuracy = accuracy_score(valid_output, y_pred)
    return accuracy

"""KNN Regression Model"""

from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import mean_squared_error

def knnRegression(train_input, train_output, valid_input, valid_output , k):
    #k = int(sqrt(sqrt(len(train_input))))
    knn = KNeighborsRegressor(n_neighbors=k)
    knn.fit(train_input, train_output)
    y_pred = knn.predict(valid_input)
    mse = mean_squared_error(valid_output, y_pred)
    return mse

"""Random Forest Classifcation Model"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report

def train_random_forest(train_input, train_output, valid_input, valid_output , n_estimators ):
    rf_classifier = RandomForestClassifier(n_estimators)
    rf_classifier.fit(train_input, train_output)
    valid_predictions = rf_classifier.predict(valid_input)
    report = classification_report(valid_output, valid_predictions, output_dict=True)
    return rf_classifier, report

"""# **Feature Selection Methods**

ANOVA correlation for feature selection
"""

from sklearn.feature_selection import SelectKBest, f_classif

def anovaFeatureSelection(train_input, train_output, num_features_to_select):
    selector = SelectKBest(score_func=f_classif, k=num_features_to_select)
    train_input_new = selector.fit_transform(train_input, train_output)
    selected_feature_indices = selector.get_support(indices=True)
    selected_feature_names = train_input.columns[selected_feature_indices]
    train_new_input_df = pd.DataFrame(train_input_new, columns=selected_feature_names)
    return selected_feature_names , train_new_input_df

"""Mutual Information classification for feature selection"""

from sklearn.feature_selection import mutual_info_classif
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd


def miClassification(train_input, train_output, valid_input, test_input, feature_precentage):
    mi_scores = mutual_info_classif(train_input, train_output)
    mi_scores = pd.Series(mi_scores, index=train_input.columns)
    mi_scores = mi_scores.sort_values(ascending=False)
    num_features_to_select = int(len(mi_scores) * feature_precentage)
    selected_features_train = mi_scores[:num_features_to_select]
    new_train_input = train_input[selected_features_train.index]
    new_valid_input = valid_input[selected_features_train.index]
    new_test_input = test_input[selected_features_train.index]
    return mi_scores , new_train_input , new_valid_input ,new_test_input, selected_features_train

def plot_mi_scores(scores):
    scores = scores.sort_values(ascending=True)
    width = np.arange(len(scores))
    ticks = list(scores.index)
    plt.barh(width, scores)
    plt.yticks(width, ticks)
    plt.title("Mutual Information Scores")


# plt.figure(dpi=100, figsize=(8, 5))
# plot_mi_scores(mi_scores)

"""Mutual information regression for feature selection"""

from sklearn.feature_selection import mutual_info_regression
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd


def miRegression(train_input, train_output, valid_input, test_input, feature_precentage):
    mi_scores = mutual_info_regression(train_input, train_output)
    mi_scores = pd.Series(mi_scores, index=train_input.columns)
    mi_scores = mi_scores.sort_values(ascending=False)
    num_features_to_select = int(len(mi_scores) * feature_precentage)
    selected_features_train = mi_scores[:num_features_to_select]
    new_train_input = train_input[selected_features_train.index]
    new_valid_input = valid_input[selected_features_train.index]
    new_test_input = test_input[selected_features_train.index]
    return mi_scores , new_train_input , new_valid_input ,new_test_input, selected_features_train

def plot_mi_scores(scores):
    scores = scores.sort_values(ascending=True)
    width = np.arange(len(scores))
    ticks = list(scores.index)
    plt.barh(width, scores)
    plt.yticks(width, ticks)
    plt.title("Mutual Information Scores")

"""# **Dimentionality Reduction Method**

PCA for dimentionality reduction
"""

from sklearn.decomposition import PCA

def performPca(train_input, valid_input,test_input, n_components):
    pca = PCA(n_components=n_components)
    train_reduced = pca.fit_transform(train_input)
    valid_reduced = pca.transform(valid_input)
    test_reduced = pca.transform(test_input)
    train_reduced_df = pd.DataFrame(train_reduced, columns=[f"new_feature_{i+1}" for i in range(train_reduced.shape[1])])
    valid_reduced_df = pd.DataFrame(valid_reduced, columns=[f"new_feature_{i+1}" for i in range(valid_reduced.shape[1])])
    test_reduced_df = pd.DataFrame(test_reduced, columns=[f"new_feature_{i+1}" for i in range(test_reduced.shape[1])])


    return train_reduced_df, valid_reduced_df,test_reduced_df

"""# **Label 1 : Speaker ID**

Splitting dataset into inputs(features) and outputs(label)
"""

train_input = train.iloc[:, :256]
label_1_train_output = train.iloc[:, 256]

valid_input = valid.iloc[:, :256]
label_1_valid_output = valid.iloc[:, 256]

test_input = test.iloc[:, :256]
label_1_test_output = valid.iloc[:, 256]

knnClassification(train_input , label_1_train_output , valid_input , label_1_valid_output,7)

"""Trying the KNN classification with several different K values I have found that k=7 gives a better accuracy level. Therefore K=7 used for the Knn model.

## **Variance Threshold**
"""

from sklearn.feature_selection import VarianceThreshold
var_thres=VarianceThreshold(threshold=0)
var_thres.fit(train_input)

var_thres.get_support()

"""We can conclude that there are no columns with constant values

## **Mutual Information For Feature Reduction**

Perform mutual information feature selection method on data set.
"""

mi_scores , new_train_input , new_valid_input , new_test_input , selected_features_train = miClassification(train_input, label_1_train_output , valid_input, test_input , 0.5)
plt.figure(dpi=100, figsize=(8, 5))
plot_mi_scores(mi_scores)

new_train_input.shape

new_valid_input.shape

new_test_input.shape

knnClassification(new_train_input , label_1_train_output , new_valid_input , label_1_valid_output,7)

"""## **PCA For Dimentionality Reduction**"""

train_reduced_df, valid_reduced_df, test_reduced_df= performPca(new_train_input, new_valid_input, new_test_input, 0.95)

train_reduced_df

valid_reduced_df

test_reduced_df

knnClassification(train_reduced_df , label_1_train_output , valid_reduced_df , label_1_valid_output,7)

"""## **Test Label 1 Data**"""

#Predicted labels before feature engineering
test_knn = KNeighborsClassifier(n_neighbors=7)
test_knn.fit(train_input, label_1_train_output)
y_pred_before = test_knn.predict(test_input)
y_pred_before

#Predicted labels after feature engineering
test_knn = KNeighborsClassifier(n_neighbors=7)
test_knn.fit(train_reduced_df, label_1_train_output)
y_pred_after = test_knn.predict(test_reduced_df)
y_pred_after

#No. of new features (total features in the final set after feature engineering and transformation)
train_reduced_df

"""Saving To CSV file"""

num_new_features_array = np.repeat(train_reduced_df.shape[1], len(y_pred_before))
# Create a DataFrame with the required columns
result_df = pd.DataFrame({
    "Predicted labels before feature engineering": y_pred_before,
    "Predicted labels after feature engineering": y_pred_after,
    "No of new features": num_new_features_array
})

num_features = train_reduced_df.shape[1]
result_df = pd.concat([result_df, test_reduced_df], axis=1)

for i in range(num_features + 1, 257):
    new_feature_col_name = f"new_feature_{i}"
    result_df[new_feature_col_name] = [None] * len(test_reduced_df)


result_df.to_csv("190451P_label_1.csv", index=False)

"""# **Label 2 : Speaker Age**"""

train_input = train.iloc[:, :256]
label_2_train_output = train.iloc[:, 257]

valid_input = valid.iloc[:, :256]
label_2_valid_output = valid.iloc[:, 257]

test_input = test.iloc[:, :256]
label_2_test_output = valid.iloc[:, 257]

"""Since age is a categorical varible , I have used KNN Regression model"""

knnRegression(train_input , label_2_train_output , valid_input , label_2_valid_output,8)

"""## **Mutual Information For Feature Reduction**"""

mi_scores , new_train_input , new_valid_input , new_test_input , selected_features_train = miRegression(train_input, label_1_train_output , valid_input, test_input , 0.5)
plt.figure(dpi=100, figsize=(8, 5))
plot_mi_scores(mi_scores)

new_train_input.shape

new_valid_input.shape

new_test_input.shape

knnRegression(new_train_input , label_2_train_output , new_valid_input , label_2_valid_output,8)

"""## **PCA For Dimentionality Reduction**"""

train_reduced_df, valid_reduced_df, test_reduced_df= performPca(new_train_input, new_valid_input, new_test_input, 0.95)

train_reduced_df

valid_reduced_df

test_reduced_df

knnRegression(train_reduced_df , label_1_train_output , valid_reduced_df , label_2_valid_output,8)

"""## **Test Label 2 Data**"""

#Predicted labels before feature engineering
test_knn = KNeighborsRegressor(n_neighbors=8)
test_knn.fit(train_input, label_2_train_output)
y_pred = test_knn.predict(test_input)
y_pred_before_2 = np.round(y_pred).astype(int)

#Predicted labels after feature engineering
test_knn = KNeighborsRegressor(n_neighbors=8)
test_knn.fit(train_reduced_df, label_2_train_output)
y_pred = test_knn.predict(test_reduced_df)
y_pred_after_2 = np.round(y_pred).astype(int)

#No. of new features (total features in the final set after feature engineering and transformation)
train_reduced_df

num_new_features_array = np.repeat(train_reduced_df.shape[1], len(y_pred_before_2))
# Create a DataFrame with the required columns
result_df = pd.DataFrame({
    "Predicted labels before feature engineering": y_pred_before_2,
    "Predicted labels after feature engineering": y_pred_after_2,
    "No of new features": num_new_features_array
})

num_features = train_reduced_df.shape[1]
result_df = pd.concat([result_df, test_reduced_df], axis=1)

for i in range(num_features + 1, 257):
    new_feature_col_name = f"new_feature_{i}"
    result_df[new_feature_col_name] = [None] * len(test_reduced_df)

result_df.to_csv("190451P_label_2.csv", index=False)

"""# **Label 3 : Speaker Gender**"""

train_input = train.iloc[:, :256]
label_3_train_output = train.iloc[:, 258]

valid_input = valid.iloc[:, :256]
label_3_valid_output = valid.iloc[:, 256]

test_input = test.iloc[:, :256]
label_3_test_output = valid.iloc[:, 258]

knnClassification(train_input , label_3_train_output , valid_input , label_3_valid_output,12)

"""## **Mutual Information For Feature Reduction**"""

mi_scores , new_train_input , new_valid_input , new_test_input , selected_features_train = miClassification(train_input, label_3_train_output , valid_input, test_input , 0.125)
plt.figure(dpi=100, figsize=(8, 5))
plot_mi_scores(mi_scores)

new_train_input.shape

knnClassification(new_train_input , label_3_train_output , new_valid_input , label_3_valid_output,12)

"""## **Test Label 3 Data**"""

#Predicted labels before feature engineering
test_knn = KNeighborsClassifier(n_neighbors=12)
test_knn.fit(train_input, label_3_train_output)
y_pred_before_3 = test_knn.predict(test_input)

#Predicted labels after feature engineering
test_knn = KNeighborsClassifier(n_neighbors=12)
test_knn.fit(new_train_input, label_3_train_output)
y_pred_after_3 = test_knn.predict(new_test_input)

#No. of new features (total features in the final set after feature engineering and transformation)
new_train_input

num_new_features_array = np.repeat(new_train_input.shape[1], len(y_pred_before_3))
# Create a DataFrame with the required columns
result_df = pd.DataFrame({
    "Predicted labels before feature engineering": y_pred_before_3,
    "Predicted labels after feature engineering": y_pred_after_3,
    "No of new features": num_new_features_array
})

num_features = new_train_input.shape[1]

for i in range(1,num_features + 1):
    new_feature_col_name = f"new_feature_{i}"
    result_df[new_feature_col_name] = new_test_input.iloc[:, i - 1]  # Assuming 0-based indexing


for i in range(num_features + 1, 257):
    new_feature_col_name = f"new_feature_{i}"
    result_df[new_feature_col_name] = [None] * len(new_test_input)

result_df.to_csv("190451P_label_3.csv", index=False)

"""# **Label 4 : Speaker Accent**"""

train_input = train.iloc[:, :256]
label_4_train_output = train.iloc[:, 259]

valid_input = valid.iloc[:, :256]
label_4_valid_output = valid.iloc[:, 259]

test_input = test.iloc[:, :256]
label_4_test_output = valid.iloc[:, 259]

knnClassification(train_input , label_4_train_output , valid_input , label_4_valid_output,7)

"""## **Data Sampling**

Data sampling is done because of the uneven distribution of the dataset
"""

from imblearn.over_sampling import SMOTE
from collections import Counter

# Apply SMOTE for oversampling
smote = SMOTE(sampling_strategy='auto', random_state=42)
sampled_train_input, sampled_train_output = smote.fit_resample(train_input, label_4_train_output)

# Visualize the class distribution after oversampling
print("Class distribution before oversampling:", Counter(label_4_train_output))
print("Class distribution after oversampling:", Counter(sampled_train_output))

knnClassification(sampled_train_input , sampled_train_output , valid_input , label_4_valid_output,7)

"""## **Mutual Information For Feature Reduction**

"""

mi_scores , new_train_input , new_valid_input , new_test_input , selected_features_train = miClassification(sampled_train_input, sampled_train_output , valid_input, test_input , 0.5)
plt.figure(dpi=100, figsize=(8, 5))
plot_mi_scores(mi_scores)

knnClassification(new_train_input , sampled_train_output , new_valid_input , label_4_valid_output , 7)

"""## **PCA For Dimentionality Reduction**"""

train_reduced_df, valid_reduced_df, test_reduced_df= performPca(new_train_input, new_valid_input, new_test_input, 0.95)

train_reduced_df

valid_reduced_df

test_reduced_df

knnClassification(train_reduced_df , sampled_train_output , valid_reduced_df , label_4_valid_output,7)

"""## **Test Label 4 Data**"""

#Predicted labels before feature engineering
test_knn = KNeighborsClassifier(n_neighbors=7)
test_knn.fit(train_input, label_4_train_output)
y_pred_before_4 = test_knn.predict(test_input)

#Predicted labels after feature engineering
test_knn = KNeighborsClassifier(n_neighbors=7)
test_knn.fit(train_reduced_df, sampled_train_output)
y_pred_after_4 = test_knn.predict(test_reduced_df)

#No. of new features (total features in the final set after feature engineering and transformation)
train_reduced_df

num_new_features_array = np.repeat(train_reduced_df.shape[1], len(y_pred_before_4))
# Create a DataFrame with the required columns
result_df = pd.DataFrame({
    "Predicted labels before feature engineering": y_pred_before_4,
    "Predicted labels after feature engineering": y_pred_after_4,
    "No of new features": num_new_features_array
})

num_features = train_reduced_df.shape[1]
result_df = pd.concat([result_df, test_reduced_df], axis=1)

for i in range(num_features + 1, 257):
    new_feature_col_name = f"new_feature_{i}"
    result_df[new_feature_col_name] = [None] * len(test_reduced_df)

result_df.to_csv("190451P_label_4.csv", index=False)